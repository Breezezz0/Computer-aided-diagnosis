{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import math\n",
    "import chardet\n",
    "from collections import OrderedDict\n",
    "import json\n",
    "import re\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = r'1110101-1110823門診患者病歷資料.xlsx'\n",
    "df = pd.read_excel(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10778\n"
     ]
    }
   ],
   "source": [
    "#資料集中有空欄位，要做處理前須先將0填入\n",
    "df = df.fillna(0)\n",
    "index_chinese = []\n",
    "for index,emr in enumerate(df.iloc[:,10]) :\n",
    "    emr = str(emr)\n",
    "    for word in emr :\n",
    "        if  (u'\\u4e00' <= word <= u'\\u9fff') : #找尋是否含有中文字\n",
    "            index_chinese.append(df.at[index,'病歷號'])\n",
    "            break\n",
    "index_chinese = list(OrderedDict.fromkeys(index_chinese))\n",
    "print(len(index_chinese))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1246\n"
     ]
    }
   ],
   "source": [
    "index_eng = []\n",
    "for index, patient in enumerate(df.iloc[:,1]) :\n",
    "    if patient not in index_chinese :\n",
    "        index_eng.append(patient)\n",
    "index_eng = list(OrderedDict.fromkeys(index_eng))\n",
    "print(len(index_eng))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#抓取病人資料並可以透過函式返回dict格式的資料\n",
    "class patient_inf ():\n",
    "    def __init__(self , df , med_rec_no) :\n",
    "        self.df = df \n",
    "        self.num = med_rec_no\n",
    "    def get_inf (self) :\n",
    "        inf = {}\n",
    "        df_patient = self.df[self.df['病歷號']==self.num]\n",
    "        inf['性別'] = df_patient.at[df_patient.index[0],'性別']\n",
    "        inf['診斷碼'] = []\n",
    "        inf['診斷碼'].append(df_patient.at[df_patient.index[0],'診斷碼'])\n",
    "        if type(df_patient.at[df_patient.index[0],'診斷碼.1']) != str :\n",
    "            inf['診斷碼'].append(df_patient.at[df_patient.index[0],'診斷碼.1'])\n",
    "        if type(df_patient.at[df_patient.index[0],'診斷碼.2']) != str :\n",
    "            inf['診斷碼'].append(df_patient.at[df_patient.index[0],'診斷碼.2'])\n",
    "        for i in range(len(df_patient.index)) :\n",
    "            if df_patient.iloc[i,9] == '徵侯' :\n",
    "                inf['徵侯'] = df_patient.iloc[i,10]\n",
    "            elif df_patient.iloc[i,9] == '病史' :\n",
    "                inf['病史'] = df_patient.iloc[i,10]\n",
    "            else :\n",
    "                inf['處置'] = df_patient.iloc[i,10]\n",
    "        return inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get patients' information from df\n",
    "dataset_inf = {}\n",
    "for patient in index_eng :\n",
    "    dataset_inf[patient] = patient_inf(df, patient).get_inf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'BW': 'Body weight', 'BH': 'Body height', 'BT': 'Body temperature', 'BMI': 'Body Mass Index', 'TOCC': 'Travel history, Occupation, Contact history, Cluster', 'Holter': '24hr Holter EKG ', 'EKG': 'Electrocardiography', 'CXR': 'Chest x-ray', 'BP': 'blood pressure', 'RHB': 'Regular heart beat', 'ER': 'emergency room', 'Tx': 'therapy', 'abd': 'abdominal', 'WBC': 'white blood cells', 'URI': 'upper respiratory tract infection', 'AGE': 'Acute Gastroenteritis', 'LV': 'left ventricle', 'EF': 'Ejection friction', 'f/u': 'follow up', 'T4': 'thyroxine', 'RBC': 'red blood cells', 'epi': 'epithelial cell of urine', 'OPD': 'Outpatient department\\xa0', 'B.S.': 'breathing sounds', 'HEERT': 'head, ear, eyes, nose, throat', 'RN': 'rhinorrhea', 'NO': 'Nasal obstruction', 'bil': 'bilateral', 'EGD': 'EsophagoGastroDuodenoscopy', 'S/P': 'Post-surgical', 'Bx': 'biopsy', 'HP': 'Helicobacter pylori', 'DRE': 'DIGITAL RECTAL EXAMINATION', 'GU': 'Gastric ulcer', 'CFS': 'Colonofibroscopy', 'DOE': 'dyspnea on exertion', 'RLL': 'right lower lobe', 'RUL': 'roght upper lobe', 'VATS': 'Video-Assisted Thoracoscopic Surgery', 'GOT': 'aspartate aminotransferase', 'GPT': 'alanine aminotransferase', 'Phx': 'Past Medical History', 'Dx': 'Diagnosis', 'Ac': 'before meals', 'Cr': 'Creatinine', 'TCHO': 'Total chosterol', 'NKDA': 'No known drug allergies', 'Vac': 'Vacuum-Assisted Closure', 'HTN': 'hypertension', 'CTA': 'Computed tomography angiography', 'C/o': 'complains of', 'RTC': 'Return to clinic', 'CVS': 'Cardiovascular Surgery', 'susp.': 'suspected', 'DM': 'diabetes mellitus', 'HBV': 'Hepatitis B\\xa0Virus', 's/s': 'Symptoms and signs', 'WNL': 'Within Normal Limits', 'GERD': 'gastroesophageal reflux disease', 'NSD': 'normal spontaneous delivery', 'eGFR': 'estimated Glomerular filtration rate', 'FM': 'Family medicine', 'IMOJEV': 'live attenuated recombinant Japanese encephalitis virus', 'VPC': 'Ventricular premature contraction', 'APC': 'atrial premature contraction', 'LMD': 'Local medical doctor', 'ST': 'ST\\xa0segment', 'DUE': 'Distal part of upper extremities', 'LE': 'lower extremities', 'Cons': 'Coagulase-negative staphylococcus', 'ICH': 'Intracerebral hemorrhage', 'CSDH': 'chronic subdural hematoma', 'HBO': 'Hyperbaric Oxygen Therapy', 'CVA': 'Cerebrovascular accident', 'LVEDD': 'left ventricular end-diastolic diameter', 'NCV': 'Nerve Conduction Velocity', 'G1P1': 'gravidity and parity', 'HIE': 'hypoxic ischemic encephalopathy', 'NE': 'neurological examination', 'RDS': 'respiratory distress syndrome', 'hx': 'History', 'Eds': 'excessive daytime sleepiness', 'ACEI': 'ACE inhibitor', 'SBP': 'systolic blood pressure', 'DC': 'Differential Count', 'Hct': 'Hematocrit', 'IDA': 'Iron-deficiency Anemia', 'KUB': 'kidney ureter bladder', 'PRN': 'as needed', 'ROM': 'Range of motion', 'RCA': 'Right coronary artery', 'PLA': 'Posterolateral artery', 'OM2': 'obtusemarginalbranch', 'CTO': 'chronic total occlusion', 'VAS': 'Visual Analogue Scale', 'NTG': 'Nitroglycerin', 'TXT': 'Treadmil-excise testing', 'OMT': 'Optimal medical therapy', 'LAD': 'left anterior descending artery', 'LCX': '\\xa0left circumflex artery', 'MP': 'MUSCLE POWER', 'ND': 'normal spontaneous delivery', 'FNA': 'Fine needle aspiration', 'mx': 'management', 'ISR': 'instent restenosis', 'SOB': 'Shortness of Breath', 'CKD': 'Chronic kidney disease', 'BNP': 'B-type Natriuretic Peptide', 'GAD': 'Generalized\\xa0anxiety disorder', 'MC': 'Menstrual Cycle', 'LMP': 'last menstrual period', 'GA': 'gestational age', 'GBS': 'Group B Streptococcus', 'tx': 'therapy', 'F/u': 'follow up', 'F/U': 'follow up', 'PET': 'positron emission tomography'}\n"
     ]
    }
   ],
   "source": [
    "# load abbr information\n",
    "df_abbr = pd.read_excel('abbreviation.xlsx')\n",
    "abbr_dict = {}\n",
    "for index , abbr in enumerate(df_abbr.iloc[:,0]) :\n",
    "    abbr_dict[abbr] = df_abbr.at[index,'Word']\n",
    "print(abbr_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    (111/01/01), nasal mucosa congested and hypert...\n",
      "1        (111/01/01) cough, sorethroat, RN for 2 days,\n",
      "2    (111/01/01)BH:87cm, BW:11kg, BMI:14.5, BT:36.7...\n",
      "3    (111/01/01), cough with sputum and rhinorrhea ...\n",
      "4                             education and medication\n",
      "Name: 病歷內容, dtype: object\n"
     ]
    }
   ],
   "source": [
    "#replace abbreviation\n",
    "df_emr = df['病歷內容'].copy()\n",
    "print(df_emr.head())\n",
    "for index, text in enumerate(df_emr) :\n",
    "    if type(text) != str :\n",
    "        df_emr[index] = str(text)\n",
    "    for abbr in abbr_dict.keys() :\n",
    "        text = re.sub(abbr,abbr_dict[abbr],str(text))\n",
    "    text = text.replace('_x000D_', '\\r')\n",
    "    df_emr[index] = str(text)\n",
    "# df_emr_changed = df_emr_changed.str.split(' ').apply(lambda x: ' '.join([abbr_dict.get(e, e) for e in x]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#做另一份複製的資料以免影響到原先的檔案\n",
    "df_inf_copy = df.copy()\n",
    "df_inf_copy['病歷內容'] = df_emr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#抽取病人資料\n",
    "dataset_inf = {}\n",
    "for patient in index_eng :\n",
    "    dataset_inf[patient] = patient_inf(df_inf_copy, patient).get_inf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save emr to excel\n",
    "df_patient = pd.DataFrame.from_dict(dataset_inf, orient='index')\n",
    "df_patient = df_patient.stack()\n",
    "# df_patient.to_excel('EMR_processed_new.xlsx')\n",
    "print(df_patient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#count 診斷碼\n",
    "diagnosis_count = {}\n",
    "for idx in dataset_inf.keys() :\n",
    "    for diag in dataset_inf[idx]['診斷碼'] :\n",
    "        diagnosis_count[diag] = diagnosis_count.get(diag,0) + 1\n",
    "print(diagnosis_count)\n",
    "#下面這行是排序\n",
    "diagnosis_count_sorted = sorted(diagnosis_count.items(),key=lambda x : x[1] ,reverse=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sorted diagnosis\n",
    "df_diag = pd.DataFrame(data=diagnosis_count_sorted,columns=['診斷碼','人數'])\n",
    "# df_diag.to_excel('各診斷碼人數.xlsx')\n",
    "print(df_diag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count 各組合\n",
    "diagnosis_count_2 = {}\n",
    "for key in dataset_inf.keys() :\n",
    "    diag_set = ','.join(str(x) for x in dataset_inf[key]['診斷碼'])\n",
    "    diagnosis_count_2[diag_set] = diagnosis_count_2.get(diag_set,0) + 1\n",
    "diagnosis_count_2_sorted = sorted(diagnosis_count_2.items(),key=lambda x : x[1] ,reverse=True)\n",
    "print(diagnosis_count_2_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_diag_set = pd.DataFrame(data=diagnosis_count_2_sorted,columns=['診斷碼','人數'])\n",
    "#df_diag_set.to_excel('emr_diag_set_new.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "from tqdm.notebook import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from ml_things import plot_dict, plot_confusion_matrix, fix_text\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from transformers import (set_seed,\n",
    "                          TrainingArguments,\n",
    "                          Trainer,\n",
    "                          GPT2Config,\n",
    "                          GPT2Model,\n",
    "                          GPT2Tokenizer,\n",
    "                          AdamW, \n",
    "                          get_linear_schedule_with_warmup,\n",
    "                          GPT2ForSequenceClassification)\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "from torchviz import make_dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocess dataset\n",
    "#remove useless data\n",
    "dataset_emr = {}\n",
    "for key in dataset_inf.keys() :\n",
    "    for diagnosis in [460, 461.9, 465.9, 466] :\n",
    "    # for diagnosis in [460, 465.9] :\n",
    "        for code in dataset_inf[key]['診斷碼'] :\n",
    "            if code == diagnosis :\n",
    "                dataset_emr[key] = dataset_inf[key]\n",
    "                break\n",
    "        if code == diagnosis :\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "991\n",
      "694\n",
      "198\n",
      "99\n"
     ]
    }
   ],
   "source": [
    "#做資料分割\n",
    "print(len(dataset_emr.keys()))\n",
    "dataset_train = {}\n",
    "dataset_valid = {}\n",
    "dataset_test = {}\n",
    "for num,keys in enumerate(dataset_emr.keys()) :\n",
    "    if num < 99 :\n",
    "        dataset_test[keys] = dataset_emr[keys]\n",
    "    elif num >= 99 and num < 793 :\n",
    "        dataset_train[keys] = dataset_emr[keys]\n",
    "    else :\n",
    "        dataset_valid[keys] = dataset_emr[keys]\n",
    "print(len(dataset_train.keys()))\n",
    "print(len(dataset_valid.keys()))\n",
    "print(len(dataset_test.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#做dataset\n",
    "#多疾病預測的資料集\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "tokenizer.padding_side = \"left\"\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "class Dataset_V2(torch.utils.data.Dataset):\n",
    "    def __init__(self, dict):\n",
    "        self.keys = dict.keys()\n",
    "        self.texts = []\n",
    "        self.labels = []\n",
    "    # Since the labels are defined by dict, we loop \n",
    "    # through each label.\n",
    "        for keys in self.keys :\n",
    "            diag_code = []\n",
    "            self.texts.append(dict[keys]['徵侯'])\n",
    "            for label in ['460', '461.9', '465.9', '466'] :\n",
    "            #for label in ['460', '465.9'] :\n",
    "                for diag in dict[keys]['診斷碼'] :\n",
    "                    if str(diag) == str(label) :\n",
    "                        diag_code.append(1)\n",
    "                        break\n",
    "                if str(diag) != str(label) :\n",
    "                    diag_code.append(0)\n",
    "            self.labels.append(diag_code)\n",
    "        # Number of exmaples.\n",
    "        self.n_examples = len(self.labels)\n",
    "        # self.labels = [labels[label] for label in self.labels]\n",
    "        self.texts = [tokenizer(text,\n",
    "                                padding='max_length',\n",
    "                                max_length=256,\n",
    "                                truncation=True,\n",
    "                                return_tensors=\"pt\") for text in self.texts]\n",
    "        \n",
    "    def classes(self):\n",
    "        return self.labels\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def get_batch_labels(self, idx):\n",
    "        # Get a batch of labels\n",
    "        return np.array(self.labels[idx])\n",
    "    \n",
    "    def get_batch_texts(self, idx):\n",
    "        # Get a batch of inputs\n",
    "        return self.texts[idx]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        batch_texts = self.get_batch_texts(idx)\n",
    "        batch_y = self.get_batch_labels(idx)\n",
    "        return batch_texts, batch_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model\n",
    "#model for 多疾病分類(二元)\n",
    "class SimpleGPT2SequenceClassifier_V2(nn.Module):\n",
    "    def __init__(self, hidden_size: int, num_classes:int ,max_seq_len:int, gpt_model_name:str):\n",
    "        super(SimpleGPT2SequenceClassifier_V2,self).__init__()\n",
    "        self.gpt2model = GPT2Model.from_pretrained(gpt_model_name)\n",
    "        self.fc1 = nn.Linear(hidden_size*max_seq_len, num_classes)\n",
    "        self.fc2 = nn.Linear(hidden_size*max_seq_len, num_classes)\n",
    "        self.fc3 = nn.Linear(hidden_size*max_seq_len, num_classes)\n",
    "        self.fc4 = nn.Linear(hidden_size*max_seq_len, num_classes)\n",
    "\n",
    "        \n",
    "    def forward(self, input_id, mask):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "                input_id: encoded inputs ids of sent.\n",
    "        \"\"\"\n",
    "        gpt_out, _ = self.gpt2model(input_ids=input_id, attention_mask=mask, return_dict=False)\n",
    "        batch_size = gpt_out.shape[0]\n",
    "        linear_output1 = self.fc1(gpt_out.view(batch_size,-1))\n",
    "        linear_output2 = self.fc2(gpt_out.view(batch_size,-1))\n",
    "        linear_output3 = self.fc3(gpt_out.view(batch_size,-1))\n",
    "        linear_output4 = self.fc4(gpt_out.view(batch_size,-1))\n",
    "        return linear_output1 , linear_output2, linear_output3, linear_output4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#個別訓練疾病種類\n",
    "def train_V2(model, train_data, val_data, learning_rate, epochs):\n",
    "    train, val = Dataset_V2(train_data), Dataset_V2(val_data)\n",
    "    \n",
    "    train_dataloader = torch.utils.data.DataLoader(train, batch_size=4, shuffle=True)\n",
    "    val_dataloader = torch.utils.data.DataLoader(val, batch_size=4)\n",
    "    \n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = Adam(model.parameters(), lr=learning_rate)\n",
    "    history_train_acc = []\n",
    "    history_valid_acc = []\n",
    "    if use_cuda:\n",
    "        model = model.cuda()\n",
    "        criterion = criterion.cuda()\n",
    "\n",
    "    for epoch_num in range(epochs):\n",
    "        total_acc_train_1 = 0\n",
    "        total_acc_train_2 = 0\n",
    "        total_acc_train_3 = 0\n",
    "        total_acc_train_4 = 0\n",
    "        total_loss_train = 0\n",
    "        \n",
    "        for train_input, train_label in tqdm(train_dataloader):\n",
    "            #train_label = train_label.to(device).to(torch.int64)\n",
    "            train_label = train_label.type(torch.LongTensor).to(device)\n",
    "\n",
    "            mask = train_input['attention_mask'].to(device)\n",
    "            input_id = train_input[\"input_ids\"].squeeze(1).to(device)\n",
    "            \n",
    "            model.zero_grad()\n",
    "\n",
    "            output1, output2, output3, output4  = model(input_id, mask)\n",
    "            \n",
    "            batch_loss1 = criterion(output1, train_label[:,0])\n",
    "            batch_loss2 = criterion(output2, train_label[:,1])\n",
    "            batch_loss3 = criterion(output3, train_label[:,2])\n",
    "            batch_loss4 = criterion(output4, train_label[:,3])\n",
    "            total_loss = (batch_loss1 + batch_loss2 + batch_loss3 + batch_loss4)/4\n",
    "            total_loss_train += total_loss.item()\n",
    "            \n",
    "            acc1 = (output1.argmax(dim=1)==train_label[:,0]).sum().item()\n",
    "            acc2 = (output2.argmax(dim=1)==train_label[:,1]).sum().item()\n",
    "            acc3 = (output3.argmax(dim=1)==train_label[:,2]).sum().item()\n",
    "            acc4 = (output4.argmax(dim=1)==train_label[:,3]).sum().item()\n",
    "            total_acc_train_1 += acc1\n",
    "            total_acc_train_2 += acc2\n",
    "            total_acc_train_3 += acc3\n",
    "            total_acc_train_4 += acc4\n",
    "\n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "        total_train_acc = [total_acc_train_1, total_acc_train_2, total_acc_train_3, total_acc_train_4]\n",
    "        history_train_acc.append(total_train_acc)\n",
    "\n",
    "        total_acc_val_1 = 0\n",
    "        total_acc_val_2 = 0\n",
    "        total_acc_val_3 = 0\n",
    "        total_acc_val_4 = 0\n",
    "        total_loss_val = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            \n",
    "            for val_input, val_label in val_dataloader:\n",
    "                val_label = val_label.type(torch.LongTensor).to(device)\n",
    "                mask = val_input['attention_mask'].to(device)\n",
    "                input_id = val_input['input_ids'].squeeze(1).to(device)\n",
    "                \n",
    "                output1, output2, output3, output4  = model(input_id, mask)\n",
    "            \n",
    "                batch_loss1 = criterion(output1, val_label[:,0])\n",
    "                batch_loss2 = criterion(output2, val_label[:,1])\n",
    "                batch_loss3 = criterion(output3, val_label[:,2])\n",
    "                batch_loss4 = criterion(output4, val_label[:,3])\n",
    "                total_loss = (batch_loss1 + batch_loss2 + batch_loss3 + batch_loss4)/4\n",
    "                total_loss_val += total_loss.item()\n",
    "                \n",
    "                acc1 = (output1.argmax(dim=1)==val_label[:,0]).sum().item()\n",
    "                acc2 = (output2.argmax(dim=1)==val_label[:,1]).sum().item()\n",
    "                acc3 = (output3.argmax(dim=1)==val_label[:,2]).sum().item()\n",
    "                acc4 = (output4.argmax(dim=1)==val_label[:,3]).sum().item()\n",
    "                total_acc_val_1 += acc1\n",
    "                total_acc_val_2 += acc2\n",
    "                total_acc_val_3 += acc3\n",
    "                total_acc_val_4 += acc4\n",
    "            total_valid_acc = [total_acc_val_1, total_acc_val_2, total_acc_val_3, total_acc_val_4]\n",
    "            history_valid_acc.append(total_valid_acc)\n",
    "                \n",
    "                \n",
    "            print(\n",
    "            f\"Epochs: {epoch_num + 1} | Train Loss: {total_loss_train/len(train_data): .3f} \\\n",
    "            | Train Accuracy1: {total_acc_train_1 / len(train_data): .3f} \\\n",
    "            | Train Accuracy2: {total_acc_train_2 / len(train_data): .3f} \\\n",
    "            | Train Accuracy3: {total_acc_train_3 / len(train_data): .3f} \\\n",
    "            | Train Accuracy4: {total_acc_train_4 / len(train_data): .3f} \\\n",
    "            | Val Loss: {total_loss_val / len(val_data): .3f} \\\n",
    "            | Val Accuracy1: {total_acc_val_1 / len(val_data): .3f} \\\n",
    "            | Val Accuracy2: {total_acc_val_2 / len(val_data): .3f} \\\n",
    "            | Val Accuracy3: {total_acc_val_3 / len(val_data): .3f} \\\n",
    "            | Val Accuracy4: {total_acc_val_4 / len(val_data): .3f} \")\n",
    "    return history_train_acc, history_valid_acc\n",
    "EPOCHS = 10\n",
    "model = SimpleGPT2SequenceClassifier_V2(hidden_size=768, num_classes=2, max_seq_len=256, gpt_model_name=\"gpt2\")\n",
    "LR = 1e-5\n",
    "\n",
    "# train_acc, valid_acc = train_V2(model, dataset_train, dataset_valid, LR, EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#繪製訓練過程\n",
    "labels = ['sinusitis', 'nasopharyngitis', 'respiratory infection', 'bronchitis&Bronchiolitis']\n",
    "colors = ['red','blue', 'purple', 'green']\n",
    "x = [i for i in range(1,EPOCHS+1)]\n",
    "for i in range(4) :\n",
    "    y = []\n",
    "    for j in range(len(train_acc)) :\n",
    "        y.append(train_acc[j][i]/len(dataset_train))\n",
    "    plt.plot(x, y, color=colors[i], linestyle=\"-\", linewidth=\"2\", markersize=\"16\", marker=\".\", label=labels[i])\n",
    "plt.legend()\n",
    "plt.title('train_acc')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#繪製validation過程\n",
    "labels = ['sinusitis', 'nasopharyngitis', 'respiratory infection', 'bronchitis&Bronchiolitis']\n",
    "colors = ['red','blue', 'purple', 'green']\n",
    "x = [i for i in range(1,EPOCHS+1)]\n",
    "for i in range(4) :\n",
    "    y = []\n",
    "    for j in range(len(valid_acc)) :\n",
    "        y.append(valid_acc[j][i]/len(dataset_valid))\n",
    "    plt.plot(x, y, color=colors[i], linestyle=\"-\", linewidth=\"2\", markersize=\"16\", marker=\".\", label=labels[i])\n",
    "plt.legend()\n",
    "plt.title('validation_acc')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SimpleGPT2SequenceClassifier_V2(\n",
       "  (gpt2model): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (fc1): Linear(in_features=196608, out_features=2, bias=True)\n",
       "  (fc2): Linear(in_features=196608, out_features=2, bias=True)\n",
       "  (fc3): Linear(in_features=196608, out_features=2, bias=True)\n",
       "  (fc4): Linear(in_features=196608, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_pre = SimpleGPT2SequenceClassifier_V2(hidden_size=768, num_classes=2, max_seq_len=256, gpt_model_name=\"gpt2\")\n",
    "model_pre.load_state_dict(torch.load(\"Gpt2-model_0206.pt\"))\n",
    "model_pre.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\anaconda3\\envs\\U-net\\lib\\site-packages\\torchviz\\dot.py:65: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if LooseVersion(torch.__version__) < LooseVersion(\"1.9\") and \\\n",
      "c:\\Users\\user\\anaconda3\\envs\\U-net\\lib\\site-packages\\torchviz\\dot.py:65: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if LooseVersion(torch.__version__) < LooseVersion(\"1.9\") and \\\n"
     ]
    },
    {
     "ename": "ExecutableNotFound",
     "evalue": "failed to execute WindowsPath('dot'), make sure the Graphviz executables are on your systems' PATH",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\U-net\\lib\\site-packages\\graphviz\\backend\\execute.py:79\u001b[0m, in \u001b[0;36mrun_check\u001b[1;34m(cmd, input_lines, encoding, quiet, **kwargs)\u001b[0m\n\u001b[0;32m     78\u001b[0m         kwargs[\u001b[39m'\u001b[39m\u001b[39mstdout\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m kwargs[\u001b[39m'\u001b[39m\u001b[39mstderr\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m subprocess\u001b[39m.\u001b[39mPIPE\n\u001b[1;32m---> 79\u001b[0m     proc \u001b[39m=\u001b[39m _run_input_lines(cmd, input_lines, kwargs\u001b[39m=\u001b[39;49mkwargs)\n\u001b[0;32m     80\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\U-net\\lib\\site-packages\\graphviz\\backend\\execute.py:99\u001b[0m, in \u001b[0;36m_run_input_lines\u001b[1;34m(cmd, input_lines, kwargs)\u001b[0m\n\u001b[0;32m     98\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_run_input_lines\u001b[39m(cmd, input_lines, \u001b[39m*\u001b[39m, kwargs):\n\u001b[1;32m---> 99\u001b[0m     popen \u001b[39m=\u001b[39m subprocess\u001b[39m.\u001b[39mPopen(cmd, stdin\u001b[39m=\u001b[39msubprocess\u001b[39m.\u001b[39mPIPE, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    101\u001b[0m     stdin_write \u001b[39m=\u001b[39m popen\u001b[39m.\u001b[39mstdin\u001b[39m.\u001b[39mwrite\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\U-net\\lib\\subprocess.py:971\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[1;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, user, group, extra_groups, encoding, errors, text, umask, pipesize)\u001b[0m\n\u001b[0;32m    968\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstderr \u001b[39m=\u001b[39m io\u001b[39m.\u001b[39mTextIOWrapper(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstderr,\n\u001b[0;32m    969\u001b[0m                     encoding\u001b[39m=\u001b[39mencoding, errors\u001b[39m=\u001b[39merrors)\n\u001b[1;32m--> 971\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_execute_child(args, executable, preexec_fn, close_fds,\n\u001b[0;32m    972\u001b[0m                         pass_fds, cwd, env,\n\u001b[0;32m    973\u001b[0m                         startupinfo, creationflags, shell,\n\u001b[0;32m    974\u001b[0m                         p2cread, p2cwrite,\n\u001b[0;32m    975\u001b[0m                         c2pread, c2pwrite,\n\u001b[0;32m    976\u001b[0m                         errread, errwrite,\n\u001b[0;32m    977\u001b[0m                         restore_signals,\n\u001b[0;32m    978\u001b[0m                         gid, gids, uid, umask,\n\u001b[0;32m    979\u001b[0m                         start_new_session)\n\u001b[0;32m    980\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[0;32m    981\u001b[0m     \u001b[39m# Cleanup if the child failed starting.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\U-net\\lib\\subprocess.py:1440\u001b[0m, in \u001b[0;36mPopen._execute_child\u001b[1;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, unused_restore_signals, unused_gid, unused_gids, unused_uid, unused_umask, unused_start_new_session)\u001b[0m\n\u001b[0;32m   1439\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1440\u001b[0m     hp, ht, pid, tid \u001b[39m=\u001b[39m _winapi\u001b[39m.\u001b[39;49mCreateProcess(executable, args,\n\u001b[0;32m   1441\u001b[0m                              \u001b[39m# no special security\u001b[39;49;00m\n\u001b[0;32m   1442\u001b[0m                              \u001b[39mNone\u001b[39;49;00m, \u001b[39mNone\u001b[39;49;00m,\n\u001b[0;32m   1443\u001b[0m                              \u001b[39mint\u001b[39;49m(\u001b[39mnot\u001b[39;49;00m close_fds),\n\u001b[0;32m   1444\u001b[0m                              creationflags,\n\u001b[0;32m   1445\u001b[0m                              env,\n\u001b[0;32m   1446\u001b[0m                              cwd,\n\u001b[0;32m   1447\u001b[0m                              startupinfo)\n\u001b[0;32m   1448\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m   1449\u001b[0m     \u001b[39m# Child is launched. Close the parent's copy of those pipe\u001b[39;00m\n\u001b[0;32m   1450\u001b[0m     \u001b[39m# handles that only the child should have open.  You need\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1453\u001b[0m     \u001b[39m# pipe will not close when the child process exits and the\u001b[39;00m\n\u001b[0;32m   1454\u001b[0m     \u001b[39m# ReadFile will hang.\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 2] 系統找不到指定的檔案。",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mExecutableNotFound\u001b[0m                        Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\U-net\\lib\\site-packages\\IPython\\core\\formatters.py:972\u001b[0m, in \u001b[0;36mMimeBundleFormatter.__call__\u001b[1;34m(self, obj, include, exclude)\u001b[0m\n\u001b[0;32m    969\u001b[0m     method \u001b[39m=\u001b[39m get_real_method(obj, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprint_method)\n\u001b[0;32m    971\u001b[0m     \u001b[39mif\u001b[39;00m method \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 972\u001b[0m         \u001b[39mreturn\u001b[39;00m method(include\u001b[39m=\u001b[39;49minclude, exclude\u001b[39m=\u001b[39;49mexclude)\n\u001b[0;32m    973\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    974\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\U-net\\lib\\site-packages\\graphviz\\jupyter_integration.py:98\u001b[0m, in \u001b[0;36mJupyterIntegration._repr_mimebundle_\u001b[1;34m(self, include, exclude, **_)\u001b[0m\n\u001b[0;32m     96\u001b[0m include \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m(include) \u001b[39mif\u001b[39;00m include \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m {\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jupyter_mimetype}\n\u001b[0;32m     97\u001b[0m include \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m \u001b[39mset\u001b[39m(exclude \u001b[39mor\u001b[39;00m [])\n\u001b[1;32m---> 98\u001b[0m \u001b[39mreturn\u001b[39;00m {mimetype: \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m, method_name)()\n\u001b[0;32m     99\u001b[0m         \u001b[39mfor\u001b[39;00m mimetype, method_name \u001b[39min\u001b[39;00m MIME_TYPES\u001b[39m.\u001b[39mitems()\n\u001b[0;32m    100\u001b[0m         \u001b[39mif\u001b[39;00m mimetype \u001b[39min\u001b[39;00m include}\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\U-net\\lib\\site-packages\\graphviz\\jupyter_integration.py:98\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     96\u001b[0m include \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m(include) \u001b[39mif\u001b[39;00m include \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m {\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jupyter_mimetype}\n\u001b[0;32m     97\u001b[0m include \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m \u001b[39mset\u001b[39m(exclude \u001b[39mor\u001b[39;00m [])\n\u001b[1;32m---> 98\u001b[0m \u001b[39mreturn\u001b[39;00m {mimetype: \u001b[39mgetattr\u001b[39;49m(\u001b[39mself\u001b[39;49m, method_name)()\n\u001b[0;32m     99\u001b[0m         \u001b[39mfor\u001b[39;00m mimetype, method_name \u001b[39min\u001b[39;00m MIME_TYPES\u001b[39m.\u001b[39mitems()\n\u001b[0;32m    100\u001b[0m         \u001b[39mif\u001b[39;00m mimetype \u001b[39min\u001b[39;00m include}\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\U-net\\lib\\site-packages\\graphviz\\jupyter_integration.py:112\u001b[0m, in \u001b[0;36mJupyterIntegration._repr_image_svg_xml\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    110\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_repr_image_svg_xml\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mstr\u001b[39m:\n\u001b[0;32m    111\u001b[0m     \u001b[39m\"\"\"Return the rendered graph as SVG string.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 112\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpipe(\u001b[39mformat\u001b[39;49m\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39msvg\u001b[39;49m\u001b[39m'\u001b[39;49m, encoding\u001b[39m=\u001b[39;49mSVG_ENCODING)\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\U-net\\lib\\site-packages\\graphviz\\piping.py:104\u001b[0m, in \u001b[0;36mPipe.pipe\u001b[1;34m(self, format, renderer, formatter, neato_no_op, quiet, engine, encoding)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpipe\u001b[39m(\u001b[39mself\u001b[39m,\n\u001b[0;32m     56\u001b[0m          \u001b[39mformat\u001b[39m: typing\u001b[39m.\u001b[39mOptional[\u001b[39mstr\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m     57\u001b[0m          renderer: typing\u001b[39m.\u001b[39mOptional[\u001b[39mstr\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     61\u001b[0m          engine: typing\u001b[39m.\u001b[39mOptional[\u001b[39mstr\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m     62\u001b[0m          encoding: typing\u001b[39m.\u001b[39mOptional[\u001b[39mstr\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m typing\u001b[39m.\u001b[39mUnion[\u001b[39mbytes\u001b[39m, \u001b[39mstr\u001b[39m]:\n\u001b[0;32m     63\u001b[0m     \u001b[39m\"\"\"Return the source piped through the Graphviz layout command.\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \n\u001b[0;32m     65\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    102\u001b[0m \u001b[39m        '<?xml version='\u001b[39;00m\n\u001b[0;32m    103\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 104\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_pipe_legacy(\u001b[39mformat\u001b[39;49m,\n\u001b[0;32m    105\u001b[0m                              renderer\u001b[39m=\u001b[39;49mrenderer,\n\u001b[0;32m    106\u001b[0m                              formatter\u001b[39m=\u001b[39;49mformatter,\n\u001b[0;32m    107\u001b[0m                              neato_no_op\u001b[39m=\u001b[39;49mneato_no_op,\n\u001b[0;32m    108\u001b[0m                              quiet\u001b[39m=\u001b[39;49mquiet,\n\u001b[0;32m    109\u001b[0m                              engine\u001b[39m=\u001b[39;49mengine,\n\u001b[0;32m    110\u001b[0m                              encoding\u001b[39m=\u001b[39;49mencoding)\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\U-net\\lib\\site-packages\\graphviz\\_tools.py:171\u001b[0m, in \u001b[0;36mdeprecate_positional_args.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    162\u001b[0m     wanted \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m=\u001b[39m\u001b[39m{\u001b[39;00mvalue\u001b[39m!r}\u001b[39;00m\u001b[39m'\u001b[39m\n\u001b[0;32m    163\u001b[0m                        \u001b[39mfor\u001b[39;00m name, value \u001b[39min\u001b[39;00m deprecated\u001b[39m.\u001b[39mitems())\n\u001b[0;32m    164\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mThe signature of \u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m will be reduced\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    165\u001b[0m                   \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m to \u001b[39m\u001b[39m{\u001b[39;00msupported_number\u001b[39m}\u001b[39;00m\u001b[39m positional args\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    166\u001b[0m                   \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlist\u001b[39m(supported)\u001b[39m}\u001b[39;00m\u001b[39m: pass \u001b[39m\u001b[39m{\u001b[39;00mwanted\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\n\u001b[0;32m    167\u001b[0m                   \u001b[39m'\u001b[39m\u001b[39m as keyword arg(s)\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m    168\u001b[0m                   stacklevel\u001b[39m=\u001b[39mstacklevel,\n\u001b[0;32m    169\u001b[0m                   category\u001b[39m=\u001b[39mcategory)\n\u001b[1;32m--> 171\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\U-net\\lib\\site-packages\\graphviz\\piping.py:121\u001b[0m, in \u001b[0;36mPipe._pipe_legacy\u001b[1;34m(self, format, renderer, formatter, neato_no_op, quiet, engine, encoding)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[39m@_tools\u001b[39m\u001b[39m.\u001b[39mdeprecate_positional_args(supported_number\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n\u001b[0;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_pipe_legacy\u001b[39m(\u001b[39mself\u001b[39m,\n\u001b[0;32m    114\u001b[0m                  \u001b[39mformat\u001b[39m: typing\u001b[39m.\u001b[39mOptional[\u001b[39mstr\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    119\u001b[0m                  engine: typing\u001b[39m.\u001b[39mOptional[\u001b[39mstr\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    120\u001b[0m                  encoding: typing\u001b[39m.\u001b[39mOptional[\u001b[39mstr\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m typing\u001b[39m.\u001b[39mUnion[\u001b[39mbytes\u001b[39m, \u001b[39mstr\u001b[39m]:\n\u001b[1;32m--> 121\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_pipe_future(\u001b[39mformat\u001b[39;49m,\n\u001b[0;32m    122\u001b[0m                              renderer\u001b[39m=\u001b[39;49mrenderer,\n\u001b[0;32m    123\u001b[0m                              formatter\u001b[39m=\u001b[39;49mformatter,\n\u001b[0;32m    124\u001b[0m                              neato_no_op\u001b[39m=\u001b[39;49mneato_no_op,\n\u001b[0;32m    125\u001b[0m                              quiet\u001b[39m=\u001b[39;49mquiet,\n\u001b[0;32m    126\u001b[0m                              engine\u001b[39m=\u001b[39;49mengine,\n\u001b[0;32m    127\u001b[0m                              encoding\u001b[39m=\u001b[39;49mencoding)\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\U-net\\lib\\site-packages\\graphviz\\piping.py:149\u001b[0m, in \u001b[0;36mPipe._pipe_future\u001b[1;34m(self, format, renderer, formatter, neato_no_op, quiet, engine, encoding)\u001b[0m\n\u001b[0;32m    146\u001b[0m \u001b[39mif\u001b[39;00m encoding \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    147\u001b[0m     \u001b[39mif\u001b[39;00m codecs\u001b[39m.\u001b[39mlookup(encoding) \u001b[39mis\u001b[39;00m codecs\u001b[39m.\u001b[39mlookup(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoding):\n\u001b[0;32m    148\u001b[0m         \u001b[39m# common case: both stdin and stdout need the same encoding\u001b[39;00m\n\u001b[1;32m--> 149\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pipe_lines_string(\u001b[39m*\u001b[39margs, encoding\u001b[39m=\u001b[39mencoding, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    150\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    151\u001b[0m         raw \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pipe_lines(\u001b[39m*\u001b[39margs, input_encoding\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoding, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\U-net\\lib\\site-packages\\graphviz\\backend\\piping.py:212\u001b[0m, in \u001b[0;36mpipe_lines_string\u001b[1;34m(engine, format, input_lines, encoding, renderer, formatter, neato_no_op, quiet)\u001b[0m\n\u001b[0;32m    206\u001b[0m cmd \u001b[39m=\u001b[39m dot_command\u001b[39m.\u001b[39mcommand(engine, \u001b[39mformat\u001b[39m,\n\u001b[0;32m    207\u001b[0m                           renderer\u001b[39m=\u001b[39mrenderer,\n\u001b[0;32m    208\u001b[0m                           formatter\u001b[39m=\u001b[39mformatter,\n\u001b[0;32m    209\u001b[0m                           neato_no_op\u001b[39m=\u001b[39mneato_no_op)\n\u001b[0;32m    210\u001b[0m kwargs \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39minput_lines\u001b[39m\u001b[39m'\u001b[39m: input_lines, \u001b[39m'\u001b[39m\u001b[39mencoding\u001b[39m\u001b[39m'\u001b[39m: encoding}\n\u001b[1;32m--> 212\u001b[0m proc \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mrun_check(cmd, capture_output\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, quiet\u001b[39m=\u001b[39mquiet, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    213\u001b[0m \u001b[39mreturn\u001b[39;00m proc\u001b[39m.\u001b[39mstdout\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\U-net\\lib\\site-packages\\graphviz\\backend\\execute.py:84\u001b[0m, in \u001b[0;36mrun_check\u001b[1;34m(cmd, input_lines, encoding, quiet, **kwargs)\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mOSError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     83\u001b[0m     \u001b[39mif\u001b[39;00m e\u001b[39m.\u001b[39merrno \u001b[39m==\u001b[39m errno\u001b[39m.\u001b[39mENOENT:\n\u001b[1;32m---> 84\u001b[0m         \u001b[39mraise\u001b[39;00m ExecutableNotFound(cmd) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n\u001b[0;32m     85\u001b[0m     \u001b[39mraise\u001b[39;00m\n\u001b[0;32m     87\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m quiet \u001b[39mand\u001b[39;00m proc\u001b[39m.\u001b[39mstderr:\n",
      "\u001b[1;31mExecutableNotFound\u001b[0m: failed to execute WindowsPath('dot'), make sure the Graphviz executables are on your systems' PATH"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x1ea0aa20610>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "model_pre.to(device)\n",
    "print(device)\n",
    "test_dataset = Dataset_V2(dataset_test)\n",
    "with torch.no_grad():\n",
    "    mask = test_dataset[1][0]['attention_mask'].to(device)\n",
    "    input_id = test_dataset[1][0]['input_ids'].squeeze(1).to(device)\n",
    "    y1, y2, y3, y4 = model_pre(input_id,mask)\n",
    "make_dot(model_pre(test_dataset[1][0]['input_ids'].squeeze(1).to(device),mask),params=dict(model_pre.named_parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'input_ids': tensor([[50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "              7, 16243,    14,   486,    14,   486,     8, 25842,  6001,    25,\n",
       "           5774, 11215,    11, 12290,  3463,    25,  1157, 10025,    11, 12290,\n",
       "           5674, 12901,    25,  1415,    13,    20,    11, 12290,  5951,    25,\n",
       "           2623,    13,    22,   158,   226,   225,   837, 13589,    25,  1729,\n",
       "            259, 35408,    13,  1027, 13026,    25, 16572,    13,  7393,    26,\n",
       "            802,   293,    13,   645, 28837, 40780, 27189,  1539, 24537, 10534,\n",
       "          36076,   290, 18443,   354,   498, 12704,  2128,    13,   645,  2612,\n",
       "           4636, 28582,  1539,  2705, 32692,  3674,    13, 12748,  1409,   540,\n",
       "          21755,    13,   645,  4168, 28509,    13]]), 'attention_mask': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])},\n",
       " array([1, 0, 0, 1]))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_V2(model, test_data):\n",
    "\n",
    "    test = Dataset_V2(test_data)\n",
    "\n",
    "    test_dataloader = torch.utils.data.DataLoader(test, batch_size=4)\n",
    "\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "    if use_cuda:\n",
    "\n",
    "        model = model.cuda()\n",
    "\n",
    "        \n",
    "    # Tracking variables\n",
    "    predictions_labels_1 = []\n",
    "    predictions_labels_2 = []\n",
    "    predictions_labels_3 = []\n",
    "    predictions_labels_4 = []\n",
    "    true_labels_1 = []\n",
    "    true_labels_2 = []\n",
    "    true_labels_3 = []\n",
    "    true_labels_4 = []\n",
    "    \n",
    "    total_acc_test_1 = 0\n",
    "    total_acc_test_2 = 0\n",
    "    total_acc_test_3 = 0\n",
    "    total_acc_test_4 = 0\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for test_input, test_label in test_dataloader:\n",
    "\n",
    "            test_label = test_label.to(device)\n",
    "            mask = test_input['attention_mask'].to(device)\n",
    "            input_id = test_input['input_ids'].squeeze(1).to(device)\n",
    "\n",
    "            output1, output2, output3, output4  = model(input_id, mask)\n",
    "            acc1 = (output1.argmax(dim=1)==test_label[:,0]).sum().item()\n",
    "            acc2 = (output2.argmax(dim=1)==test_label[:,1]).sum().item()\n",
    "            acc3 = (output3.argmax(dim=1)==test_label[:,2]).sum().item()\n",
    "            acc4 = (output4.argmax(dim=1)==test_label[:,3]).sum().item()\n",
    "            total_acc_test_1 += acc1\n",
    "            total_acc_test_2 += acc2\n",
    "            total_acc_test_3 += acc3\n",
    "            total_acc_test_4 += acc4\n",
    "            \n",
    "            # add original labels\n",
    "            true_labels_1 += test_label[:,0].cpu().numpy().flatten().tolist()\n",
    "            true_labels_2 += test_label[:,1].cpu().numpy().flatten().tolist()\n",
    "            true_labels_3 += test_label[:,2].cpu().numpy().flatten().tolist()\n",
    "            true_labels_4 += test_label[:,3].cpu().numpy().flatten().tolist()\n",
    "            # get predicitons to list\n",
    "            predictions_labels_1 += output1.argmax(dim=1).cpu().numpy().flatten().tolist()\n",
    "            predictions_labels_2 += output2.argmax(dim=1).cpu().numpy().flatten().tolist()\n",
    "            predictions_labels_3 += output3.argmax(dim=1).cpu().numpy().flatten().tolist()\n",
    "            predictions_labels_4 += output4.argmax(dim=1).cpu().numpy().flatten().tolist()\n",
    "    \n",
    "    print(f'Test Accuracy: {((total_acc_test_1+total_acc_test_2+total_acc_test_3+total_acc_test_4) /4)/len(test_data): .3f}')\n",
    "    return [true_labels_1,true_labels_2,true_labels_3,true_labels_4], [predictions_labels_1,predictions_labels_2,predictions_labels_3,predictions_labels_4]   \n",
    "true_labels, pred_labels = evaluate_V2(model, dataset_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrix.\n",
    "labels = {'True':1 ,'False':0}\n",
    "fig, ax = plt.subplots(2,2,figsize=(10, 10))\n",
    "for i in range(4) :\n",
    "    cm = confusion_matrix(y_true=true_labels[i], y_pred=pred_labels[i], labels=range(len(labels)))\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=list(labels.keys()))\n",
    "    disp.plot(ax=ax[i//2][i%2])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "U-net",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2e776d25b26044f877e66ccacd3eafff1d5f630652572c876190bd1cd39c11a7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
